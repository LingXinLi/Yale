---
title: '365'
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning = FALSE}
#LingXin Li(Cynthia Li) sta365 final exam
###If you are running my rmd code for grading, and the image(jpg) cannot be loaded, that is because the path to image is in my own computer(problem3 parta), and so does the data(eg.the code of loading data is also to the path of my own computer). Please see the knitted file I hand in in quercus.

#I installed tinytex to this, but when I knit it to pdf, it came with error said that I have not installed it, so that I knit to the html/word document, and then convert word to pdf.
library(R2jags)
library(rjags)
library(runjags)
library(MCMCpack)
library(lattice)
library(MASS)
library(tidyverse)
library(dplyr)
library(tinytex)


```


```{r}
#Problem1 handwriting, please see the other file.
```




\newpage
##Problem2

a. Write your choices of the parameters clearly.

$\mu_1 = 1$
$\mu_2 = 4$
$\delta = 0.5$
$\sigma^2 = 9$


```{r}
###Problem2
#b. Produce the code used to generate the simulations.
set.seed(1000) 
N <- 1000
mu1 <- 1
mu2 <- 4
delta <- 0.5
sigma2 <- 9
#rnorm(n, mean, sd)
Z1 <- rnorm(N, mu1, sqrt(sigma2))
Z2 <- rnorm(N, mu2, sqrt(sigma2))
Y <- delta * Z1 + (1 - delta) * Z2

```

\newpage
```{r}
###Problem2
#c. Plot a histogram of your simulated data.
hist(Y)
```
\newpage
```{r}
###Problem2
#d. Overlay a plot of the density of the variable Y on the histogram.

#I used the following reference(the section of "Density" and "Random Variates" of this references link)to help me understand better about how to rnorm() and dnorm() in order to simulated data and calculate the pdf for the above and below code for problem 2, and also the function of seq: 

#Probability Distributions in R (Stat 5101, Geyer). (n.d.). https://www.stat.umn.edu/geyer/old/5101/rlook.html#:~:text=dnorm%20is%20the%20R%20function,standard%20deviation%20of%20the%20distribution.



x <- seq(min(Y), max(Y), length.out = 1000) 
hist(Y,  probability = TRUE)
lines(x, dnorm(x, mean = mean(Y), sd = sd(Y)), col = "blue" )

```
\newpage
```{r}
###Problem2


#e. Label the graph clearly, using captions or titles that mention your parameter choices.

hist(Y, 
     main = "Histogram of simulated data consisting of iid draws from Y \n (N = 1000, mu1 = 1, mu2 = 4, delta = 0.5, sigma^2 = 9)", 
     cex.main = 0.91,
     xlab = "Y", probability = TRUE)
lines(x, dnorm(x, mean = mean(Y), sd = sd(Y)), col = "blue")
legend("topright", legend = c("Density of the variable \n Y =  δZ1 +(1− δ)Z2"), col = "blue", lty = 1)

```


\newpage
##Problem3

```{r}
##Problem3
#Part(a) if you cannot see the picture of answer here if you are running the rmd(since the path to the image file is in my own computer, I save the image and rmd in the same path of my computer), please my knitted file hand in quercus
###HANDWRITING IMAGE FOR MY ANSWER IS IN MY KNITTED FILE IN QUERCUS

knitr::include_graphics("p3pa.jpg")

```



```{r, warning=FALSE}
##Problem3
#source of data of problem 3
#Gelman, & Hill. (n.d.). wells.dat. http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat. Retrieved April 13, 2023, from http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat

#Part (b)

wells <- read.table("wells.dat.txt", header = TRUE, col.names = c("nth_obs", "switch", "arsenic", "dist", "assoc", "educ"))

#arsenic levels and distance is Xi. Center all X-variables.
#The following code let each xi minus xbar
mean_arsenic <- mean(wells$arsenic)
mean_distance <- mean(wells$dist)
wells$arsenic_center <- wells$arsenic - mean_arsenic
wells$dist_center <- wells$dist - mean_distance

```


```{r}
##Problem3
#Part (b)
#write a JAGS model

#The following references is 
#Hu, J. a. a. J. (2020, July 30). Chapter 12 Bayesian Multiple Regression and Logistic Models | Probability and Bayesian Modeling. https://bayesball.github.io/BOOK/bayesian-multiple-regression-and-logistic-models.html#bayesian-logistic-regression

#2nd reference is lecture materials

#according to my references and the given information of the question, it said that Fi is a linear function of the predictors xi, theta i can also be expressed in p since Yi~BIN(n,theta(is also p of success)). Then Fi is equals to log(theta i/(1-theta i)) which is given by the function, and it is equals to beta0+beta1xi1+beta2xi2(because we have 2 xi in this question which is arsenic levels and distance from the wells). After we simiplied, theta i = exp(beta0+beta1xi1+beta2xi2) / (1 + exp(beta0+beta1xi1+beta2xi2) where theta lies between 0 and 1.


set.seed(1000)
JAGS_logistic = function() {
  # Likelihood
  for (i in 1:n) {
    Y[i] ~ dbin(theta[i],1) #yi ∼ Binomial(Ni,theta i )
    f[i] ~ dnorm(X[i,] %*% beta, tau2_logistic) #This is Fi∼ N(Xibeta,sigma^2 ), and according to week lecture I use tau2 for sigma2 position
    theta[i] <- exp(f[i]) / (1 + exp(f[i])) #interpreted according to my references, also we can use the function ilogit(x) refer to Rdocumentation
  }
  
  # Prior for beta
  for (j in 1:p) {
    beta[j] ~ dnorm(0.0, tau2_logistic_beta) #Given that beta ∼ N(0,(sigma_beta)^2*I), and I use tau2 for sigma_beta square according to the professor's lecture materials. 
    
  }
  
  tau2_logistic <- 1.0/sigma2 #refer to professor's lecture week8B sigma2 <- 1.0/tau2, and I convert it
  sigma2 ~ dgamma(nu0/2, nu0*sigma0_square/2) #Given that σ^2 ∼ Inverse-Gamma(ν0/2,ν0*σ0^2/2), but I failed to run sigma2 ~dinvgamma(nu0/2, nu0*sigma0_sq/2), so that I think it should be dgamma

}

```





```{r}
##Problem3
#Part (b)
Y <- wells$switch #well switching as the yi 
X <- model.matrix(switch ~ arsenic_center + dist_center, data = wells) #function refer to RDocumentation
n <- nrow(wells)
nu0 <- 1
sigma0_square <- 1
sigma2 <- 1
tau2_logistic_beta <- 0.0001
p <- 3 #intercept and predictors: beta0, beta1, beta2
set.seed(1000)
data.JAGS = list(Y = Y, 
                  X = X, 
                  n = n, 
                  p = p,
                  tau2_logistic_beta = tau2_logistic_beta,
                  nu0 = nu0, 
                  sigma0_square = sigma0_square)

inits.JAGS = list(list(beta = rnorm(p, 0, 1), sigma2 = sigma2))

para.JAGS = c("beta", "sigma2")

```


```{r}
##Problem3
#Part (b)
#References: lecture materials
set.seed(1000)
fit.JAGS.logistic = jags(data=data.JAGS,
                inits=inits.JAGS,
                parameters.to.save = para.JAGS,
                n.chains=1,
                n.iter=10000,
                n.burnin=1000,
                model.file=JAGS_logistic)

```

```{r}
##Problem3
#Part (b)
#References: lecture materials
print(fit.JAGS.logistic)
```

beta[1] here represent beta0 which is our intercept.

beta[2] here actually represent beta1 which is the first predictor arsenic levels(which I already centered). 95% credible intervals for beta 1 is we looked at the section"beta[2]", look at the 2.5% and 97.5 quantiles. So that the 95% credible intervals for beta 1 is [0.394, 0.571]

beta[3] here actually represent beta2 which is the second predictor distance from the wells(which I already centered by using xi-xbar). 95% credible intervals for beta 1 is we looked at the section"beta[2]", look at the 2.5% and 97.5 quantiles. So that the 95% credible intervals for beta 1 is [-0.012, -0.007]

look at the sigma2 section, 95% credible intervals for sigma square is [0.050,0.617] which is also according to 2.5% and 97.5 quantiles.



```{r}
#References: lecture materials
##Problem3
#Part (c)
traceplot(fit.JAGS.logistic,mfrow=c(2,3),ask=FALSE)
```


Note that beta[1] here represent beta0 which is our intercept, beta[2] here actually represent beta1 which is the first predictor arsenic levels(which I already centered), beta[3] here actually represent beta2 which is the second predictor distance from the wells(which I already centered by using xi-xbar).

(I looked at their examples of traceplots to help me understand better about some criteria/good example/bad example for traceplots: 
I looked into the website of bad examples here Evaluation of MCMC samples. (n.d.). Cross Validated. https://stats.stackexchange.com/questions/311151/evaluation-of-mcmc-samples
The above user of the website led me to here:
Evaluating Markov Chain Monte Carlo (MCMC) Algorithms. (n.d.). https://link.springer.com/content/pdf/10.1007/978-0-387-71265-9_6.pdf

and I have also looked at Bakker, R. (n.d.). Florida State University Bayesian Workshop. https://spia.uga.edu/faculty_pages/rbakker/bayes/Day2/Day2_Convergence.pdf)

I generated traceplots. I believe MCMC moderately converge to a posterior, and their mixing is fine. All of the traceplot for beta showed above tend to have moderate number of flutuations, where partial pattern and partial trends looks deviate from the majority pattern. In other words, some trends look unstable here, but overall, I would say it did provide moderately strong evidence of convergence.

\newpage
##Problem4
```{r}
###Problem4
load("C:/Bayes/swim_time.RData")
swim_time <- get(load('C:/Bayes/swim_time.RData'))
library(reshape2)
```

```{r}

###Problem4
#use of melt in reshape2 package citation is according to the reference of R. (n.d.). and reference Zach. (2022) which you can see in the references page.

Y$Swimmer <- factor(1:4)

swim_time1 <- melt(Y, 
                   id.vars = "Swimmer", 
                   variable.name = "Week", 
                   value.name = "Time")

swim_time1$Week <- parse_number(as.character(swim_time1$Week))


#mean of range 22 to 24 is 23, variance of range 22 to 24 is ((22 - 23)^2 + (23 - 23)^2 + (24 - 23)^2) / 2 = 1
#many the following code is initially from the professor lecture materials with my modification

para.JAGS <- c("alpha", "beta", "tau2", "sigma2")
set.seed(1000)
linear.model.JAGS = function(){
  for(i in 1:n){
    y[i] ~ dnorm(mu[i],tau2)
    mu[i]<- alpha + beta*(x[i]-x.bar) 
  }
  x.bar <- mean(x)
  alpha ~ dnorm(23, 1)
  beta ~ dnorm(0.0, 1.0E-4)
  sigma2 <- 1.0/tau2
  tau2 ~ dgamma(0.1,0.1)
}



```


```{r}
###Problem4
lst0 <- list()

swim_time2 <- swim_time1 %>% group_by(Swimmer)
set.seed(1000)
for (j in 1:4) {
  new_data <- swim_time2 %>% filter(Swimmer == j)
  y <- new_data$Time
  x <- new_data$Week
  n <- length(x)  
  
#references: professor lecture materials 
  data.JAGS = list(y = y, x = x, n = n)
  inits.JAGS = list(list(alpha = 23.0, beta = 0.0, tau2 = 1.0))
  set.seed(1000)
  fit.JAGS = jags(data = data.JAGS,
                   inits = inits.JAGS,
                   parameters.to.save = para.JAGS,
                   n.chains = 1,
                   n.iter = 9000,
                   n.burnin = 1000,
                   model.file = linear.model.JAGS)
  
  lst0[[j]] <- fit.JAGS
}
```
\newpage
```{r}
#references: professor lecture materials
#traceplot for swimmer1
traceplot(lst0[[1]],mfrow=c(2,3),ask=FALSE)
```
\newpage
```{r}
#traceplot for swimmer2
#references: professor lecture materials
traceplot(lst0[[2]],mfrow=c(2,3),ask=FALSE)
```
\newpage
```{r}
#traceplot for swimmer3
#references: professor lecture materials
traceplot(lst0[[3]],mfrow=c(2,3),ask=FALSE)
```
\newpage
```{r}
#traceplot for swimmer4
#references: professor lecture materials
traceplot(lst0[[4]],mfrow=c(2,3),ask=FALSE)
```

\newpage
##Problem4 continued
1. comment on the suitability of the resulting model and Whether we have reached MCMC convergence to a posterior

(I looked at their examples of traceplots to help me understand better about some criteria/good example/bad example for traceplots: 
I looked into the website of bad examples here Evaluation of MCMC samples. (n.d.). Cross Validated. https://stats.stackexchange.com/questions/311151/evaluation-of-mcmc-samples
The above user of the website led me to here:
Evaluating Markov Chain Monte Carlo (MCMC) Algorithms. (n.d.). https://link.springer.com/content/pdf/10.1007/978-0-387-71265-9_6.pdf

and I have also looked at Bakker, R. (n.d.). Florida State University Bayesian Workshop. https://spia.uga.edu/faculty_pages/rbakker/bayes/Day2/Day2_Convergence.pdf)

my answer:
A traceplot indicated how the value of each parameter has changed across iterations of the chain. First, ignored all graph of deviance since it is not one of our parameters. Second, for all four swimmers, all of the traceplots based on each parameter roughly reached MCMC converged to a posterior. Because I observed rare big fluctuations in the pattern. However, if we look at it in a more strict way, I would say that there are more big fluctuations in all traceplots of all 4 swimmers in sigma square, but I think it should still be considered as a stable pattern which values converging around a certain point. The non-existence of big fluctuations indicates that most of the values are within similar range, each values are not going so far from the average(or other statistical values) of the distribution which means they are moving around similar points without much deviation. 

Overall, all of the traceplots for all swimmers based on each parameter can be considered as good convergence(and or good mixing), one obvious thing is that there is no special trends that deviate from the majority.

2. Whether my priors are reasonable. 

my answer:
I think my prior is reasonable since it is based on the information that already existed which is 22 to 24 seconds is the competitive times range for this age group.
mean of range 22 to 24 is 23, variance of range 22 to 24 is ((22 - 23)^2 + (23 - 23)^2 + (24 - 23)^2) / 2 = 1, so standard deviation is the square root of variance which is one. The prior is only on the alpha which is on the intercept since I do not have the information on beta, tau and sigma. Therefore the prior for beta, tau and sigma are noninformative priors which their posterior is heavily rely on the data.

3. comment on how I would revising the model, and how I would evaluate if this revised model is better than the current version

my answer:

First, according to Evaluating Markov Chain Monte Carlo (MCMC) Algorithms. (n.d.). https://link.springer.com/content/pdf/10.1007/978-0-387-71265-9_6.pdf, covergence could be influenced by many factors, for example, the initial values for the parameters. With that being said, I think we could improve our priors for each parameter, but it required more information which we do not have this time. Second, we use swimming time as the response variable and week as the explanatory variable, we could use other predictors to predict y next time, there should more things that are related to a swimmer's swimming time. According to week11A lecture of the professor, we learned many methods of model assessment. We could use Bayes factors, cross validation, Deviance information criteria(DIC) to compared our initial and revised model so that we could select the best one. 

\newpage
##Problem5

```{r}
###Problem5
data(UScrime)
#Part (a)
#references: professor lecture materials
set.seed(1000)
JAGS_BLR_flat = function(){
  # Likelihood
  for(i in 1:n){
    Y[i] ~ dnorm(mu[i],inv_sigma2)
    mu[i] <- beta_0 + inprod(X[i,],beta) 
    # same as beta_0 + X[i,1]*beta[1] + ... + X[i,p]*beta[p]
  }
  # Prior for beta
  for(j in 1:p){
    beta[j] ~ dnorm(0,0.0001)
    #non-informative priors 
  }
  # Prior for intercept
  beta_0 ~ dnorm(0, 0.0001)
  
  # Prior for the inverse variance
  inv_sigma2 ~ dgamma(0.0001, 0.0001)
  sigma2 <- 1.0/inv_sigma2
}



```


```{r}
###Problem5
#Part (a)
set.seed(1000)
mydat <- setNames(list(
  UScrime$y,
  UScrime[,-16],
  nrow(UScrime),
  ncol(UScrime[,-16])
), c("Y", "X", "n", "p"))
p <- mydat$p
```

```{r}
###Problem5
#Part (a)
#references: professor lecture materials
set.seed(1000)
fit_JAGS_flat = jags(data=mydat,
                inits=list(list(beta = rnorm(p),
                                beta_0 = 0,
                                inv_sigma2 = 1)),
                parameters.to.save = c("beta_0","beta","sigma2"), 
                n.chains=1,
                n.iter=10000,
                n.burnin=1000,
                model.file=JAGS_BLR_flat)

```
\newpage
```{r}
###Problem5
#Part (a)
#references: professor lecture materials
print(fit_JAGS_flat)
```


\newpage
```{r}
###Problem5
#Part (a)
#references: professor lecture materials
fit_flat =as.mcmc(fit_JAGS_flat)
summary(fit_flat)
```

1.
(ignored the deviance, beta0, sigma2 since they are not our predictors, I keep them for more detailed information)
Look at the output in "1. Empirical mean and standard deviation for each variable, plus standard error of the mean", 
   
   the column of "Mean" represent the marginal posterior mean for each of beta[i].


2. 
(ignored the deviance, beta0, sigma2 also)
Look at the output in "2. Quantiles for each variable:",

 95% credible intervals only need the quantiles of 2.5% and 97.5%. We look at the column of 2.5% and 97.5% for each beta[i]. In confidence interval in the frequent statistics where it is more likely to find no relationship of variables after you run the experiment one more time if the confidence interval includes zero. I think Bayesian could be similar to this criteria which indicates that if my 95% credible interval excludes zero, then we reject the null hypothesis assuming that there is no linear relationship between crimes and a certain explanatory variable beta[i]. In other words, there is a linear relationship between crimes and that certain explanatory variable if 95% credible interval excludes zero. According to the output, the following variables seem strongly predictive
of crime rates:

beta[13]  95% credible interval is [0.3376,1.129e+01] 
beta[4]   95% credible interval is [0.4713,5.250e+01]

(If the number is different when running the rmd, that may due to R studio problem, because I have set seed for my simulations, it should be the same number)

beta[4] is police expenditure in 1960, so that police expenditure in 1960 seems strongly predictive
of crime rates.

beta[13] is income inequality, so that income inequality seems strongly predictive
of crime rates.



\newpage
##Problem5 Partb
```{r}
#references: professor lecture materials
set.seed(1000)
JAGS_BLR_SpikeSlab = function(){
  # Likelihood
  for(i in 1:n){
    Y[i] ~ dnorm(mu[i],inv_sigma2)
    mu[i] <- beta_0 + inprod(X[i,],beta) 
  }
  # Prior for beta
  for(j in 1:p){
    beta[j] ~ dnorm(0,inv_tau2[j])
    inv_tau2[j] <- (1-gamma[j])*1000+gamma[j]*0.01
    gamma[j] ~ dbern(0.5)
  }
  # Prior for intercept
  beta_0 ~ dnorm(0, 0.0001)
  
  # Prior for the inverse variance
  inv_sigma2 ~ dgamma(0.0001, 0.0001)
  sigma2 <- 1.0/inv_sigma2
  tau2 <- 1.0/inv_tau2
}


```

\newpage
```{r}
###Problem5
#Part (b)
set.seed(1000)
mydat1 <- setNames(list(
  UScrime$y,
  UScrime[,-16],
  nrow(UScrime),
  ncol(UScrime[,-16])
), c("Y", "X", "n", "p"))
p <- mydat1$p
```

```{r}
###Problem5
#references: professor lecture materials
#Part (b)
set.seed(1000)
fit_JAGS_SpikeSlab = jags(data=mydat1,
                       inits=list(list(beta = rnorm(p),
                                       beta_0 = 0,
                                       inv_sigma2 = 1,
                                       gamma = rep(1,length=p))),
                       parameters.to.save = c("beta","gamma"),
                       n.chains=1,
                       n.iter=10000,
                       n.burnin=1000,
                       model.file=JAGS_BLR_SpikeSlab)
```

```{r}
###Problem5
#Part (b)
#references: professor lecture materials
print(fit_JAGS_SpikeSlab)
```
\newpage
```{r}
###Problem5
#Part (b)
#references: professor lecture materials
fit_SpikeSlab =as.mcmc(fit_JAGS_SpikeSlab)
summary(fit_SpikeSlab)
```

1.
(ignored the deviance and gamma since they are not our predictors, I keep them for more detailed model information)
Look at the output in "1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:", 
   
   the column of "Mean" represent the marginal posterior mean for each of beta[i].


2. 
(ignored the deviance and gamma also)
Look at the output in "2. Quantiles for each variable:",

None of the 95% credible interval excludes zero, so that none of the explanatory variable beta[i] seem strongly predictive of crime rates.


\newpage
References

Bakker, R. (n.d.). Florida State University Bayesian Workshop. https://spia.uga.edu/faculty_pages/rbakker/bayes/Day2/Day2_Convergence.pdf

Evaluating Markov Chain Monte Carlo (MCMC) Algorithms. (n.d.). https://link.springer.com/content/pdf/10.1007/978-0-387-71265-9_6.pdf

Evaluation of MCMC samples. (n.d.). Cross Validated. https://stats.stackexchange.com/questions/311151/evaluation-of-mcmc-samples

Gelman, & Hill. (n.d.). wells.dat. http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat. Retrieved April 13, 2023, from http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat

Home - RDocumentation. (n.d.). https://www.rdocumentation.org/

Hu, J. a. a. J. (2020, July 30). Chapter 12 Bayesian Multiple Regression and Logistic Models | Probability and Bayesian Modeling. https://bayesball.github.io/BOOK/bayesian-multiple-regression-and-logistic-models.html#bayesian-logistic-regression

Melting data | R. (n.d.). DataCamp. https://campus.datacamp.com/courses/abc-intro-2-r/data-wrangling?ex=2

Probability Distributions in R (Stat 5101, Geyer). (n.d.). https://www.stat.umn.edu/geyer/old/5101/rlook.html#:~:text=dnorm%20is%20the%20R%20function,standard%20deviation%20of%20the%20distribution.

Zach. (2022). How to Use the melt() Function in R. Statology. https://www.statology.org/melt-in-r/